{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMHD3rHCXFWIw7ipBpX89aW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/coursewarefactory/NLP_training/blob/main/Simple_Training_NLP_to_Tranlsate_one_word.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#1. Import necessary libraries:\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "#2. Load the data:\n",
        "word_pairs = [(\"love\", \"кохання\")]\n",
        "\n",
        "#3. Data preprocessing:\n",
        "# Convert words to lowercase and remove special characters\n",
        "word_pairs = [(x.lower(), y.lower()) for x, y in word_pairs] # here x and y should be in lower case\n",
        "\n",
        "# Create vocabulary and assign each word a unique index\n",
        "vocab = {word: i for i, word in enumerate(set([pair[0] for pair in word_pairs] + [pair[1] for pair in word_pairs]))}\n",
        "\n",
        "#4. Create the Encoder and Decoder structure:\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_size, embedding_size, hidden_size):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "        self.gru = nn.GRU(embedding_size, hidden_size)\n",
        "\n",
        "    def forward(self, input):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        output, hidden = self.gru(embedded)\n",
        "        return output, hidden\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, embedding_size, hidden_size, output_size):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(output_size, embedding_size)\n",
        "        self.gru = nn.GRU(embedding_size, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        output, hidden = self.gru(embedded, hidden)\n",
        "        output = self.softmax(self.out(output[0]))\n",
        "        return output, hidden\n",
        "\n",
        "#5. Embedding layer:\n",
        "# Define the size of the vocabulary and the embedding size\n",
        "vocab_size = len(vocab)\n",
        "embedding_size = 100\n",
        "hidden_size = 100\n",
        "\n",
        "# Initialize the encoder and decoder\n",
        "encoder = Encoder(vocab_size, embedding_size, hidden_size)\n",
        "decoder = Decoder(embedding_size, hidden_size, vocab_size)\n",
        "\n",
        "#6. Training the model:\n",
        "# Define the loss function and the optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(list(encoder.parameters()) + list(decoder.parameters()), lr=0.1) # include decoder parameters also in optimizer\n",
        "\n",
        "# Train the model for 100 epochs\n",
        "for epoch in range(100):\n",
        "    for pair in word_pairs:\n",
        "        input_word = pair[0]\n",
        "        target_word = pair[1]\n",
        "\n",
        "        # Encode input word and feed into decoder\n",
        "        encoder_input = torch.tensor([vocab[input_word]])\n",
        "        encoder_output, encoder_hidden = encoder(encoder_input)\n",
        "\n",
        "        # Decode and get output word\n",
        "        decoder_hidden = encoder_hidden\n",
        "        decoder_input = torch.tensor([[vocab[target_word]]])\n",
        "        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
        "\n",
        "        # Calculate loss and perform gradient descent\n",
        "        loss = criterion(decoder_output, torch.tensor([vocab[target_word]]))\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "#7. Testing the model:\n",
        "# Test the model by giving it an unseen input word \"passion\"\n",
        "encoder_input = torch.tensor([vocab[\"love\"]])\n",
        "encoder_output, encoder_hidden = encoder(encoder_input)\n",
        "decoder_hidden = encoder_hidden\n",
        "decoder_input = torch.tensor([[0]])  # Start of sequence token\n",
        "decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
        "\n",
        "# Get output word from decoder\n",
        "output_word = list(vocab.keys())[list(vocab.values()).index(decoder_output.argmax().item())]\n",
        "print(output_word)  # Output: кохання"
      ],
      "metadata": {
        "id": "rqIzpO-HNJtx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}